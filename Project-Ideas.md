Here are some project ideas, roughly in the order of increasing difficulty / scope. For all projects that involve applying synthesis in a new domain, the *default* is to use an existing synthesis framework such as Sketch, Rosette, or PROSE, but you can also implement a synthesizer from scratch if you feel up to task. Please talk to us before deciding on your project topic.

* **CEGIS:** Re-implement a symbolic program synthesizer based on a CEGIS loop. Your synthesizer must accept input in the [Sygus](http://sygus.org/) format
* **VSA:** Re-implement a basic version of FlashFill based on VSAs. Possibly extend it to other string operations and/or beyond strings.
* **Synthesis + architecture:** Use synthesis to enhance the URISC approach to fault tolerance. [This paper](http://ieeexplore.ieee.org/document/6679035) proposes to re-implement faulty instructions on a chip using a single Turing-complete instruction `subleq`. However, they had to come up with `subleq`-encodings for all instructions by hand; we could automate it using synthesis. Furthermore, instead of using `subleq` (which requires a special URISC co-processor) we can try to encode faulty instructions using other instructions available on the same chip.
* **Synthesis for homomorphic encryption:** Homomorphic encryption allows us to compute directly on encrypted, but those computations are restricted essentially to polynomials (e.g. no conditionals or loops are allowed). In this project you will convert programs with loops (such as image processing programs) into polynomials using program synthesis. We recommend using Sketch or Rosette.
* **Synthesis for lambda calculus:** This project is about synthesizing terms in (pure, untyped) lambda calculus from examples of evaluation. For instance, given examples like `INC ZERO ~> ONE` and `INC ONE ~> TWO`, together with the definitions of `ZERO`, `ONE`, and `TWO` in the Church encoding, the synthesizer has to infer that `INC` is `\n f x -> f (n f x)`. In the advanced version of this project, the Church encoding can also be inferred. We recommend that you build the synthesizer on top of [Elsa](https://github.com/ucsd-progsys/elsa).
* **Extensions to Synquid:**
    1. Currently, Synquid generates programs without `let`-expressions. Build an alternative version of Synquid that generates programs in A-normal form. Compare performance with the original version.
    2. Build an extension that provides the user with useful information when synthesis fails. Useful information might include the set of most likely incomplete solutions and the set of most likely missing components.
* **Synthesis for liquid types:** [LiquidHaskell](https://ucsd-progsys.github.io/liquidhaskell-blog/) can automatically infer refinement types for your Haskell functions. Unfortunately, when the function is recursive, it requires the user to provide *qualifiers*: atomic predicates to be used as building blocks for unknown refinements. This hinders the usability of LiquidHaskell, since the programmer now has to guess which qualifiers are required to verify a desired property. In this project you will treat the unknown refinements as a program, and use program synthesis techniques to generate them automatically, without relying on qualifiers. A simplified version of this project still relies on qualifiers, but those qualifiers can have unknown constants.
* **Synthesis + databases:** In the context of [coordination avoidance](www.vldb.org/pvldb/vol8/p185-bailis.pdf), database replicas can diverge for a while and then their states have to be merged. Can we use synthesis to automatically generate the merging function given the description of the data and the operations that can be performed on it?
* **Generic equivalence reduction:** Equivalence reduction is a popular technique for pruning the search space in program synthesis, where the synthesizer discard programs that behave equivalently to some previously explored program. Existing reduction techniques are based either on observational equivalence or on a hard-coded set of equivalences, like `x + 0 = x`. The goal of this project is to develop a more general equivalence reduction technique that figures out the equivalences automatically from the logical specifications of components (e.g. from their refinement types).
* **De-normalization:** Synthesizers often generate programs in some *normal form*, which is different from the natural way a programmer would write it (the simplest example: all variables names are x1, x2, x3...). The goal of this project is to use statistical models learned from a corpus of code in order to *de-normalize* such a synthesized program and make it look more natural while maintaining the same behavior.
* **Synthesis of test generators:** Smart test input generators, such as [type-targeted testing](https://link.springer.com/chapter/10.1007%2F978-3-662-46669-8_33) user SMT-solvers to come up with interesting test inputs for a program based on its specification. With this approach, however, generating each input is expensive, since it involves constraint solving. Instead, we could take the specification of the program-under-test and synthesize a *generator function* that efficiently emits a stream of valid inputs.
