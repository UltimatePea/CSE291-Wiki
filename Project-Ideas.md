Here are some project ideas, roughly in the order of increasing difficulty / scope. The list is likely to be extended. Please talk to us before deciding on your project topic.

* Use one the frameworks (Sketch/Rosette/PROSE) to implement a synthesizer for a new DSL
* **Synthesis + architecture:** Use synthesis to enhance the URISC approach to fault tolerance. [This paper](http://ieeexplore.ieee.org/document/6679035) proposes to re-implement faulty instructions on a chip using a single Turing-complete instruction `subleq`. However, they had to come up with `subleq`-encodings for all instructions by hand; we could automate it using synthesis. Furthermore, instead of using `sebleq` (which requires a special URISC co-processor) we can try to encode faulty instructions using other instructions available on the same chip.
* **Synthesis for liquid types:** [LiquidHaskell](https://ucsd-progsys.github.io/liquidhaskell-blog/) can automatically infer refinement types for your Haskell functions. Unfortunately, when the function is recursive, it requires the user to provide *qualifiers*: atomic predicates to be used as building blocks for unknown refinements. This hinders the usability of LiquidHaskell, since the programmer now has to guess which qualifiers are required to verify a desired property. In this project you will treat the unknown refinements as a program, and use program synthesis techniques to generate them automatically, without relying on qualifiers.
* **Synthesis of distributed programs:** [Bakst et al OOPLA'17](http://abakst.github.io/oopsla17.pdf) has shown how to automatically *sequentialize* a distributed program in order to prove its correctness. What if do the opposite, i.e. take a sequential program and run their sequentialization procedure in reverse in order to produce a distributed program? The project can have various levels of difficulty: in the simplest case, the synthesizer only has to generate appropriate message passing instructions; in a more complex case it is also responsible for splitting the work between the processes.
* **Synthesis + databases:** In the context of [coordination avoidance](www.vldb.org/pvldb/vol8/p185-bailis.pdf), database replicas can diverge for a while and then their states have to be merged. Can we use synthesis to automatically generate the merging function given the description of the data and the operations that can be performed on it?
* **Generic equivalence reduction:** Equivalence reduction is a popular technique for pruning the search space in program synthesis, where the synthesizer discard programs that behave equivalently to some previously explored program. Existing reduction techniques are based either on observational equivalence or on a hard-coded set of equivalences, like `x + 0 = x`. The goal of this project is to develop a more general equivalence reduction technique that figures out the equivalences automatically from the logical specifications of components (e.g. from their refinement types).
* "De-normalization": Use probabilistic models (PHOGs?) to made a program generated by a specification-based synthesizer more readable
* Synthesizing test generators for refinement types (instead of solving constraints at run time like in [type-targeted testing](https://link.springer.com/chapter/10.1007%2F978-3-662-46669-8_33), synthesize a program that efficiently emits a stream of valid inputs)
* Use program synthesis to implement generic Stream Fusion (not limited to a predefined library of functions on lists)
* Apply ideas from Sketch-n-Sketch to create Excel-style plot generation tools, except the generated plots would be in TikZ/R/... and a wide range of their parameters would be configurable through direct manipulation
