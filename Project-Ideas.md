Here are some project ideas, roughly in the order of increasing difficulty / scope. The list is likely to be extended. Please talk to us before deciding on your project topic.

* Use one the frameworks (Sketch/Rosette/PROSE) to implement a synthesizer for a new DSL
* **Synthesis + architecture:** Use synthesis to enhance the URISC approach to fault tolerance. [This paper](http://ieeexplore.ieee.org/document/6679035) proposes to re-implement faulty instructions on a chip using a single Turing-complete instruction `subleq`. However, they had to come up with `subleq`-encodings for all instructions by hand; we could automate it using synthesis. Furthermore, instead of using `sebleq` (which requires a special URISC co-processor) we can try to encode faulty instructions using other instructions available on the same chip.
* **Synthesis for liquid types:** [LiquidHaskell](https://ucsd-progsys.github.io/liquidhaskell-blog/) can automatically infer refinement types for your Haskell functions. Unfortunately, when the function is recursive, it requires the user to provide *qualifiers*: atomic predicates to be used as building blocks for unknown refinements. This hinders the usability of LiquidHaskell, since the programmer now has to guess which qualifiers are required to verify a desired property. In this project you will treat the unknown refinements as a program, and use program synthesis techniques to generate them automatically, without relying on qualifiers.
* **Synthesis of distributed programs:** [Bakst et al OOPLA'17](http://abakst.github.io/oopsla17.pdf) has shown how to automatically *sequentialize* a distributed program in order to prove its correctness. What if do the opposite, i.e. take a sequential program and run their sequentialization procedure in reverse in order to produce a distributed program? The project can have various levels of difficulty: in the simplest case, the synthesizer only has to generate appropriate message passing instructions; in a more complex case it is also responsible for splitting the work between the processes.
* **Synthesis + databases:** In the context of [coordination avoidance](www.vldb.org/pvldb/vol8/p185-bailis.pdf), database replicas can diverge for a while and then their states have to be merged. Can we use synthesis to automatically generate the merging function given the description of the data and the operations that can be performed on it?
* **Generic equivalence reduction:** Equivalence reduction is a popular technique for pruning the search space in program synthesis, where the synthesizer discard programs that behave equivalently to some previously explored program. Existing reduction techniques are based either on observational equivalence or on a hard-coded set of equivalences, like `x + 0 = x`. The goal of this project is to develop a more general equivalence reduction technique that figures out the equivalences automatically from the logical specifications of components (e.g. from their refinement types).
* **De-normalization:** Synthesizers often generate programs in some *normal form*, which is different from the natural way a programmer would write it (the simplest example: all variables names are x1, x2, x3...). The goal of this project is to use statistical models learned from a corpus of code in order to *de-normalize* such a synthesized program and make it look more natural while maintaining the same behavior.
* **Synthesis of test generators:** Smart test input generators, such as [type-targeted testing](https://link.springer.com/chapter/10.1007%2F978-3-662-46669-8_33) user SMT-solvers to come up with interesting test inputs for a program based on its specification. With this approach, however, generating each input is expensive, since it involves constraint solving. Instead, we could take the specification of the program-under-test and synthesize a *generator function* that efficiently emits a stream of valid inputs.
* **Generic stream fusion:** [Stream fusion](https://dl.acm.org/citation.cfm?id=1291199) is crucial to the efficiency of modern functional programming languages. It translates programs like `foldr1 (+) . filter (> 0) . map (+ 1)`, which seems to require multiple passes and several intermediate lists, into a single list traversal. However, stream fusion is based on predefined libraries that provide stream versions of the list combinators like `filter` and `map`, and hence user-defined list combinators cannot be fused. The goal of this project is to use program synthesis to generate stream versions for arbitrary user-defined functions on lists (or alternatively, synthesize fused versions of whole list programs directly).
* **Resource-guided synthesis:** Existing synthesizers either don't take performance of the generated programs into account, or (like superoptimization techniques) only target straight-line programs. In this project you will integrate Synquid with type-based [resource analysis](http://www.raml.co/) to enable Synquid to generate programs that are not only provably correct, but also provably efficient.
* **Synthesis with separation logic:** Separation logic is a popular program logic for reasoning about heap-manipulating imperative program and the underlying technology behind [Facebook's Infer](http://fbinfer.com/). The goal of this project is to generate low-level, yet provably safe C programs from separation logic specifications. One practical use case is generating a destructor for a complex linked data structure.