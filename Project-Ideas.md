Here are some project ideas, roughly in the order of increasing difficulty / scope. The list is likely to be extended. Please talk to us before deciding on your project topic.

* Use one the frameworks (Sketch/Rosette/PROSE) to implement a synthesizer for a new DSL
* **Synthesis + architecture:** Use synthesis to enhance the URISC approach to fault tolerance. [This paper](http://ieeexplore.ieee.org/document/6679035) proposes to re-implement faulty instructions on a chip using a single Turing-complete instruction `subleq`. However, they had to come up with `subleq`-encodings for all instructions by hand; we could automate it using synthesis. Furthermore, instead of using `sebleq` (which requires a special URISC co-processor) we can try to encode faulty instructions using other instructions available on the same chip.
* **Synthesis for liquid types:** [LiquidHaskell](https://ucsd-progsys.github.io/liquidhaskell-blog/) can automatically infer refinement types for your Haskell functions. Unfortunately, when the function is recursive, it requires the user to provide *qualifiers*: atomic predicates to be used as building blocks for unknown refinements. This hinders the usability of LiquidHaskell, since the programmer now has to guess which qualifiers are required to verify a desired property. In this project you will treat the unknown refinements as a program, and use program synthesis techniques to generate them automatically, without relying on qualifiers.
* **Synthesis of distributed programs:** [Bakst et al OOPLA'17](http://abakst.github.io/oopsla17.pdf) has shown how to automatically *sequentialize* a distributed program in order to prove its correctness. What if do the opposite, i.e. take a sequential program and run their sequentialization procedure in reverse in order to produce a distributed program? The project can have various levels of difficulty: in the simplest case, the synthesizer only has to generate appropriate message passing instructions; in a more complex case it is also responsible for splitting the work between the processes.
* (Synthesis + databases) In the context of [coordination avoidance](www.vldb.org/pvldb/vol8/p185-bailis.pdf), automatically synthesize functions that merge database replicas that have diverged
* Implement generic equivalence reduction based on specifications of components (e.g. refinement types) rather than a predefined set of equivalences
* "De-normalization": Use probabilistic models (PHOGs?) to made a program generated by a specification-based synthesizer more readable
* Synthesizing test generators for refinement types (instead of solving constraints at run time like in [type-targeted testing](https://link.springer.com/chapter/10.1007%2F978-3-662-46669-8_33), synthesize a program that efficiently emits a stream of valid inputs)
* Use program synthesis to implement generic Stream Fusion (not limited to a predefined library of functions on lists)
* Apply ideas from Sketch-n-Sketch to create Excel-style plot generation tools, except the generated plots would be in TikZ/R/... and a wide range of their parameters would be configurable through direct manipulation
